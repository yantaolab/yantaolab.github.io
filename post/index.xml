<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | HCIC Lab</title>
    <link>https://yantaolab.github.io/post/</link>
      <atom:link href="https://yantaolab.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 14 May 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yantaolab.github.io/media/logo_hu7225832d305736d33d1ee86989ca8ec6_178698_300x300_fit_lanczos_3.png</url>
      <title>Posts</title>
      <link>https://yantaolab.github.io/post/</link>
    </image>
    
    <item>
      <title>New publication to AiC! An unsupervised low-light image enhancement method for improving V-SLAM localization in uneven low-light construction sites</title>
      <link>https://yantaolab.github.io/post/04_2024_xiniyu_aic/</link>
      <pubDate>Tue, 14 May 2024 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/post/04_2024_xiniyu_aic/</guid>
      <description>&lt;p&gt;Construction robots have been increasingly adopted in construction projects to improve productivity, reduce risk, and speed up work cycles. Visual Simultaneous Localization and Mapping (V-SLAM) technology is widely used in construction robots due to its lightweight weight, cost-effectiveness, and provision of semantic information. On real construction sites, lighting conditions are often challenging due to factors such as uneven lighting intensity, inadequate exposure, or robots in backlit positions (e.g. roughcast houses without artificial lighting, underground car parks), which cause images captured under these conditions to exhibit low signal-to-noise ratio, uneven lighting, color distortion, noise, and other problems. These challenges make the extraction and matching of feature points in V-SLAM difficult, resulting in significant positioning errors or the inability to generate positioning trajectories for construction robotics. Although existing methods for enhancing low-light images can certainly improve image brightness, they still cannot effectively address the localization challenges brought about by low-light construction scenes with uneven illumination and noise. To address the interference of uneven and changing lighting conditions on V-SLAM localization in construction sites, this paper proposes the Unsupervised Reflectance Retinex and Noise model (URRN-Net) to enhance low-light construction images. By using a CNN model based on the Retinex theory to decompose the illumination characteristics of images, we achieve effective brightness restoration of uneven low-light images by utilizing unsupervised loss functions. URRE-Net can reduce the root mean square localization error by &amp;gt;65% compared to low-light images in the ORB-SLAM3 method, and the maximum error is reduced by &amp;gt;71% in on-site experiments. The proposed URRE-Net can be integrated with existing V-SLAM algorithms to provide more robust localization services for low-light construction site applications such as building operations (e.g., interior wall spraying robotic) or construction management tasks (e.g., automatic tunnel inspection).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning multi-granular worker intentions from incomplete visual observations for worker-robot collaboration in construction</title>
      <link>https://yantaolab.github.io/post/11_2023_zaolin_aic/</link>
      <pubDate>Sun, 14 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/post/11_2023_zaolin_aic/</guid>
      <description>&lt;p&gt;Recognizing a worker&amp;rsquo;s intentions is an important prerequisite to enable smooth human-robot collaboration in construction. However, the highly dynamic construction workplace and long-horizon construction tasks prevent robots from obtaining long-term observations, which is detrimental to information accumulation and intention disambiguation. We present (1) a data and knowledge fusion strategy that combines visual contextual information and task knowledge to reduce the information loss caused by incomplete observations, and (2) a multi-granularity worker intent recognition model to explore the optimal granularity by comparing the intent modeling capabilities of different granularities. Results show that the proposed method can recognize multi-granular worker intentions with macro-F1 scores higher than 0.85, and that the intermediate activity is the best-suited granularity as it strikes a good balance between intention recognition accuracy and intention modeling capability. For more details, please read our paper: &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0926580523004442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.sciencedirect.com/science/article/pii/S0926580523004442&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Our paper &#34;Recovering Building Information Model from 2D Drawings for Mechanical, Electrical and Plumbing Systems of Ageing Buildings&#34; has been accepted by Automation in Construction!</title>
      <link>https://yantaolab.github.io/post/0429_zaolin_aic/</link>
      <pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/post/0429_zaolin_aic/</guid>
      <description>&lt;p&gt;Ageing buildings have become a significant concern for many cities, exacerbated by inadequate management and maintenance of Mechanical, Electrical, and Plumbing (MEP) systems, and Building Information Modelling (BIM) enables efficient MEP Operation and Maintenance (O&amp;amp;M) through the digital representation of system information; however, many ageing buildings were constructed without BIM, and manual reconstruction is costly and inefficient due to the sheer number of such structures. Although some studies have proposed methods for automatically recovering BIM from 2D drawings, few are suitable for MEP systems due to the multiscale and irregular shapes of MEP components. To fill this gap, an automatic approach is proposed for recovering MEP BIM from 2D drawings with three modules: 1) semantic extraction by combining image cropping with Cascade Mask R-CNN to detect and segment multiscale, irregular MEP components; 2) geometric extraction by semantic-assisted image processing to extract contours and skeletons of irregular parts; and 3) Industry Foundation Class (IFC)-based BIM reconstruction via the open-source pythonOCC and IfcOpenShell. The performance was tested on two MEP systems with 335 and 282 multiscale and irregular elements, and the results show that the method recovered BIMs for the two MEP systems in 2.85 seconds and 0.79 seconds, with semantic extraction accuracy exceeding 0.9 and geometric error below 5%. This paper contributes to the existing body of knowledge by providing a semantic and geometric-based approach for recovering multiscale and irregular components from 2D drawings. Future studies could further improve the approach by integrating elevation drawings, reconstructing abstract symbols, and aligning text-geometry.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Our team received &#34;Best Paper - The Honorable mention&#34; award from the NSFC-RGC 2023.</title>
      <link>https://yantaolab.github.io/post/nsfc-rgc2023/</link>
      <pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/post/nsfc-rgc2023/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://yantaolab.github.io/img/post/featured1.jpg&#34; alt=&#34;pic&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
