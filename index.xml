<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HCIC Lab</title>
    <link>https://yantaolab.github.io/</link>
      <atom:link href="https://yantaolab.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>HCIC Lab</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yantaolab.github.io/media/logo_hu7225832d305736d33d1ee86989ca8ec6_178698_300x300_fit_lanczos_3.png</url>
      <title>HCIC Lab</title>
      <link>https://yantaolab.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://yantaolab.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Individualized exoskeleton design for construction workers based on biomechanical digital twins</title>
      <link>https://yantaolab.github.io/project/exoskeleton/</link>
      <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/project/exoskeleton/</guid>
      <description>&lt;p&gt;The construction task requires high physical strength of construction workers and is easy to lead to physical fatigue of construction workers. Many construction tasks typically result in accidents and work musculoskeletal disorders (WMSD). The heavy workload will affect the safety and health of workers and will also exacerbate the labor shortage in the construction industry in many countries and regions, including Hong Kong. The exoskeleton, as a wearable external mechanical structure, may benefit workers by reducing their physical workload and improving their productivity. My research focuses on developing a biomechanical digital twin to assist the individualized design of exoskeletons for construction workers.&lt;/p&gt;
&lt;p&gt;This research would apply a non-intrusive method to collect the kinematic data and establish a musculoskeletal model for construction workers. The method combines computer vision technology which is a 3D motion capture algorithm using an RGB camera and smart insoles for construction workers which is capable of monitoring the reaction force of feet generated by a work pattern. The potential, feasibility, and applicability of industrial exoskeletons applied to construction workers will be analyzed. Based on the musculoskeletal model, the biomechanical analysis will be carried out to identify exceptionally loaded positions, analyze representative muscle groups and joint loads of each pose representative of the model, find over-strained body areas and add exoskeletons. Then propose a model-based framework will be proposed to optimize and evaluate the individual exoskeleton design for construction workers. And, using the model-based approach to extend worker actions from poses to movement sequences. The research will accelerate the application of exoskeleton for construction workers.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vision-based low-light construction workers&#39; pose analysis</title>
      <link>https://yantaolab.github.io/project/lowlight/</link>
      <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/project/lowlight/</guid>
      <description>&lt;p&gt;Construction in the low-light environment is widely conducted in many construction projects, such as nighttime road construction and tunnel construction. Workers are also exposed to higher risks at low light due to dazzling lights and fatigable low-light environments. Estimating human pose can help assess workload and reduce the risk of fatigue and injury in low light. However, the existing method cannot accurately identify the Workers&amp;rsquo; pose in the low light environment. As a result, this paper proposes an Unsupervised Illumination Reflectance Estimation Network (UIRE-Net) framework for estimating Workers&amp;rsquo; poses in low light. The image restoration method is based on the Retinex theory, in which objects&amp;rsquo; true color is determined solely by illumination reflectance. To begin, the Retinex method recovers the information from low-light images. The Workers&amp;rsquo; pose estimation module is then used to track the worker&amp;rsquo;s pose. The proposed method&amp;rsquo;s pose estimation performance is evaluated using 200 low-light construction images and over 1000 Workers&amp;rsquo; pose estimation experiments. The results show that UIRE-Net has a recognition rate of 77.9%. In the low-light construction scenario, the proposed method outperforms the baseline method by 17.8%. To improve productivity and safety performance, UIRE-Net can estimate Workers&amp;rsquo; pose in low light and assist in completing automatic monitoring tasks in low-light construction.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automated Selection and Localization of Mobile Cranes in Construction Planning</title>
      <link>https://yantaolab.github.io/publication/hongling20220429buildings/</link>
      <pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/hongling20220429buildings/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Heart rate variability based physical exertion monitoring for manual material handling tasks</title>
      <link>https://yantaolab.github.io/publication/waleed20220426ergon/</link>
      <pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/waleed20220426ergon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quantifying the Effect of Mental Stress on Physical Stress for Construction Tasks</title>
      <link>https://yantaolab.github.io/publication/waleed20220301asceco/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/waleed20220301asceco/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Three-Dimensional Working Pose Estimation in Industrial Scenarios With Monocular Camera</title>
      <link>https://yantaolab.github.io/publication/yantao20210201jiot/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/yantao20210201jiot/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Posture-related data collection methods for construction workers: A review</title>
      <link>https://yantaolab.github.io/publication/yantao20210122autcon/</link>
      <pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/yantao20210122autcon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Welcome to Wowchemy, the website builder for Hugo</title>
      <link>https://yantaolab.github.io/post/getting-started/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/post/getting-started/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li&gt;
&lt;li&gt;The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong&gt;no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong&gt; and having &lt;strong&gt;flexibility to later add even deeper personalization with HTML and CSS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/main/starters/academic/preview.png&#34; alt=&#34;The template is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;get-started&#34;&gt;Get Started&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üëâ &lt;a href=&#34;https://wowchemy.com/templates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Create a new site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìö &lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Personalize your site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üí¨ &lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;Wowchemy community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üê¶ Twitter: &lt;a href=&#34;https://twitter.com/wowchemy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23MadeWithWowchemy&amp;amp;src=typed_query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üí° &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt; for &lt;em&gt;Wowchemy&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;‚¨ÜÔ∏è &lt;strong&gt;Updating Wowchemy?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/hugo-tutorials/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Tutorial&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/h2&gt;
&lt;p&gt;To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p&gt;
&lt;h3 id=&#34;-click-here-to-become-a-sponsor-and-help-support-wowchemys-future-httpswowchemycomsponsor&#34;&gt;&lt;a href=&#34;https://wowchemy.com/sponsor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy&amp;rsquo;s future ‚ù§Ô∏è&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As a token of appreciation for sponsoring, you can &lt;strong&gt;unlock &lt;a href=&#34;https://wowchemy.com/sponsor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these&lt;/a&gt; awesome rewards and extra features ü¶Ñ‚ú®&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wowchemy/hugo-academic-cli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic CLI&lt;/a&gt;:&lt;/strong&gt; Automatically import publications from BibTeX&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration&#34;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://wowchemy.com/user-stories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://wowchemy.com/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://wowchemy.com/docs/import/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://wowchemy.com/docs/install-locally/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://wowchemy.com/docs/customization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Wowchemy and its templates come with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/customization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully customizable.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Construction Activity Recognition and Ergonomic Risk Assessment Using a Wearable Insole Pressure System</title>
      <link>https://yantaolab.github.io/publication/maxwell20200701asceco/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/maxwell20200701asceco/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Joint-Level Vision-Based Ergonomic Assessment Tool for Construction Workers</title>
      <link>https://yantaolab.github.io/publication/yantao20190505asceco/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/yantao20190505asceco/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automatic Biomechanical Workload Estimation for Construction Workers by Computer Vision and Smart Insoles</title>
      <link>https://yantaolab.github.io/publication/yantao20190503ascecp/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/yantao20190503ascecp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An automatic and non-invasive physical fatigue assessment method for construction workers</title>
      <link>https://yantaolab.github.io/publication/yantao20190312autcon/</link>
      <pubDate>Tue, 12 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/yantao20190312autcon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Wearable insole pressure system for automated detection and classification of awkward working postures in construction workers</title>
      <link>https://yantaolab.github.io/publication/maxwell20181016autcon/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/maxwell20181016autcon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing and Understanding Workers‚Äô Activities in Far-Field Surveillance Videos with Deep Action Recognition and Bayesian Nonparametric Learning</title>
      <link>https://yantaolab.github.io/publication/xiaocun20181008mice/</link>
      <pubDate>Mon, 08 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/xiaocun20181008mice/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quantifying the physical intensity of construction workers, a mechanical energy approach</title>
      <link>https://yantaolab.github.io/publication/liulin20180831aei/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/liulin20180831aei/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automatic Pixel-Level Crack Detection and Measurement Using Fully Convolutional Network</title>
      <link>https://yantaolab.github.io/publication/xincong20180829mice/</link>
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/xincong20180829mice/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Estimating Construction Workers&#39; Physical Workload by Fusing Computer Vision and Smart Insole Technologies</title>
      <link>https://yantaolab.github.io/publication/yantao2018isarc/</link>
      <pubDate>Sun, 29 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/yantao2018isarc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards efficient and objective work sampling: Recognizing workers&#39; activities in site surveillance videos with two-stream convolutional networks</title>
      <link>https://yantaolab.github.io/publication/xiaocun20180725autcon/</link>
      <pubDate>Wed, 25 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/xiaocun20180725autcon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Image-and-Skeleton-Based Parameterized Approach to Real-Time Identification of Construction Workers‚Äô Unsafe Behaviors</title>
      <link>https://yantaolab.github.io/publication/hongling20180606asceco/</link>
      <pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/hongling20180606asceco/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A deep learning-based method for detecting non-certified work on construction sites</title>
      <link>https://yantaolab.github.io/publication/fang20180228aei/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/fang20180228aei/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An experimental study of real-time identification of construction workers&#39; unsafe behaviors</title>
      <link>https://yantaolab.github.io/publication/yantao20170808autcon/</link>
      <pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/yantao20170808autcon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The availability of wearable-device-based physical data for the measurement of construction workers&#39; psychological status on site: From the perspective of safety management</title>
      <link>https://yantaolab.github.io/publication/hongling20170612autcon/</link>
      <pubDate>Mon, 12 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/hongling20170612autcon/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Visualization technology-based construction safety management: A review </title>
      <link>https://yantaolab.github.io/publication/hongling20161129autcon/</link>
      <pubDate>Tue, 29 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/publication/hongling20161129autcon/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://yantaolab.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
