<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Intention | HCIC Lab</title>
    <link>https://yantaolab.github.io/tag/intention/</link>
      <atom:link href="https://yantaolab.github.io/tag/intention/index.xml" rel="self" type="application/rss+xml" />
    <description>Intention</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 15 Jan 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yantaolab.github.io/media/logo_hu7225832d305736d33d1ee86989ca8ec6_178698_300x300_fit_lanczos_3.png</url>
      <title>Intention</title>
      <link>https://yantaolab.github.io/tag/intention/</link>
    </image>
    
    <item>
      <title>Learning multi-granular worker intentions from incomplete visual observations for worker-robot collaboration in construction</title>
      <link>https://yantaolab.github.io/project/intention/</link>
      <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://yantaolab.github.io/project/intention/</guid>
      <description>&lt;p&gt;Recognizing a worker&amp;rsquo;s intentions is an important prerequisite to enable smooth human-robot collaboration in construction. However, the highly dynamic construction workplace and long-horizon construction tasks prevent robots from obtaining long-term observations, which is detrimental to information accumulation and intention disambiguation. We present (1) a data and knowledge fusion strategy that combines visual contextual information and task knowledge to reduce the information loss caused by incomplete observations, and (2) a multi-granularity worker intent recognition model to explore the optimal granularity by comparing the intent modeling capabilities of different granularities. Results show that the proposed method can recognize multi-granular worker intentions with macro-F1 scores higher than 0.85, and that the intermediate activity is the best-suited granularity as it strikes a good balance between intention recognition accuracy and intention modeling capability. Dataset is available &lt;a href=&#34;https://drive.google.com/drive/folders/1KZjStvZv-JVeTAmy27Mv-U9LkGaQS8gL?usp=drive_link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. For more details, please read our &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0926580523004442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
